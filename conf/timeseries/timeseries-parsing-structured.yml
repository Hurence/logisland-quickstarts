version: 1.1.2
documentation: LogIsland future factory job

engine:
  component: com.hurence.logisland.engine.spark.KafkaStreamProcessingEngine
  configuration:
    spark.app.name: TimeseriesParsing
    spark.master: local[4]
    spark.driver.memory: 4G
    spark.streaming.batchDuration: 2000
    spark.streaming.kafka.maxRatePerPartition: 10000
    spark.streaming.timeout: -1
  
  controllerServiceConfigurations:

    - controllerService: file_service
      component: com.hurence.logisland.stream.spark.structured.provider.LocalFileStructuredStreamProviderService
      configuration:
        local.input.path: /opt/logisland/data/timeseries

    - controllerService: console_service
      component: com.hurence.logisland.stream.spark.structured.provider.ConsoleStructuredStreamProviderService

    - controllerService: kafka_service
      component: com.hurence.logisland.stream.spark.structured.provider.KafkaStructuredStreamProviderService
      configuration:
        kafka.input.topics: logisland_raw
        kafka.output.topics: logisland_measures
        kafka.error.topics: logisland_errors
        kafka.input.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.output.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.metadata.broker.list: kafka:9092
        kafka.zookeeper.quorum: zookeeper:2181
        kafka.topic.autoCreate: true
        kafka.topic.default.partitions: 4
        kafka.topic.default.replicationFactor: 1

    - controllerService: kafka_service_out
      component: com.hurence.logisland.stream.spark.structured.provider.KafkaStructuredStreamProviderService
      configuration:
        kafka.input.topics: logisland_measures
        kafka.output.topics: logisland_metrics
        kafka.error.topics: logisland_errors
        kafka.input.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.output.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.metadata.broker.list: kafka:9092
        kafka.zookeeper.quorum: zookeeper:2181
        kafka.topic.autoCreate: true
        kafka.topic.default.partitions: 4
        kafka.topic.default.replicationFactor: 1        

    - controllerService: datastore_service
      component: com.hurence.logisland.service.solr.Solr8ClientService
      configuration:
        solr.cloud: false
        solr.connection.string: http://solr1:8983/solr
        solr.collection: historian
        solr.concurrent.requests: 4
        flush.interval: 1000
        batch.size: 200

  streamConfigurations:

    # This stream take all raw events as lines comming from local files
    # these lines are split into logisland records and sent into a kafka topic
    - stream: parsing_stream
      component: com.hurence.logisland.stream.spark.structured.StructuredStream
      configuration:
        read.topics.serializer: none
        read.stream.service.provider: kafka_service
        write.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        write.stream.service.provider: kafka_service
      processorConfigurations:

        - processor: historian_parser
          component: com.hurence.logisland.processor.SplitText
          configuration:
            record.type: historian_serie
            value.regex: (\S+\s+\S+);(\S+);(\S+);(\S+)
            value.fields: record_time,tagname,record_value,quality

        - processor: create_aliases
          component: com.hurence.logisland.processor.NormalizeFields
          configuration:
            conflict.resolution.policy: keep_both_fields
            record_name: tagname

        - processor: fields_types_converter
          component: com.hurence.logisland.processor.ConvertFieldsType
          configuration:
            record_value: double
            quality: float

    # This stream will perform a statefull groupBy operation on tagname
    - stream: compaction_stream
      component: com.hurence.logisland.stream.spark.structured.StructuredStream
      configuration:
        read.topics.key.serializer: com.hurence.logisland.serializer.StringSerializer
        read.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        read.stream.service.provider: kafka_service_out
        write.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        write.stream.service.provider: kafka_service_out
        groupby: tagname
        chunk.size: 50
        state.timeout.ms: 30000

      processorConfigurations:

        # Make one chronix chunk from all records
        - processor: timeseries_converter
          component: com.hurence.logisland.processor.ConvertToTimeseries
          configuration:
            groupby: tagname
            metric: max;min;avg;count;trend;sax:7,0.01,10;sum;first

        # Make one chronix chunk from all records
        - processor: add name
          component: com.hurence.logisland.processor.AddFields
          configuration:
            name: ${record_name}

        # Make one chronix chunk from all records
        - processor: convert chunk value into base64 string
          component: com.hurence.logisland.processor.EncodeBase64
          configuration:
            source.fields: chunk_value
            destination.fields: chunk_value
        # all the parsed records are added to solr by bulk
        - processor: solr_publisher
          component: com.hurence.logisland.processor.datastore.BulkPut
          configuration:
            datastore.client.service: datastore_service